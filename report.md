Vercel AI SDK エージェントモード徹底調査（2025年9月時点）

1. エージェントモードとは

Vercel の AI SDK では、大規模言語モデル（LLM）と外部ツールを組み合わせてタスクを自律的に実行するための仕組みを エージェントモード として提供している。エージェントは LLM がタスクの流れを計画し、必要なツールを呼び出しながら徐々にゴールへ到達するプロセスである ￼。ひとつの ループ が会話履歴や文脈を保持しながら、モデルが生成したツール呼び出しを実行し、結果をモデルへ戻す。この繰り返し（stopWhen 条件に達するまで続く）こそがエージェントモードの中核であり、単発の回答生成とは異なる長期的なタスク実行が可能となる ￼。

1.1 エージェントコアの構成要素
	1.	モデル – GPT‑5 などの LLM。AI SDK では generateText/streamText/generateObject 等が提供され、エージェントループ内でこれらを利用する。
	2.	ツール – モデルがアクセスできる外部機能。関数名・説明・入力スキーマを定義し、execute 関数で実装する。AI SDK ではツールの呼び出し結果を自動的にモデルへ渡し、マルチステップでループさせる機能がある ￼。
	3.	コンテキスト管理 – 会話履歴や過去のツール呼び出し結果を保持し、適切な長さに圧縮する機構。prepareStep や onStepFinish でコンテキストを加工してループを制御できる ￼。

1.2 ツール定義とマルチステップ

ツールは name、description、inputSchema を持ち、JSON スキーマや Zod で引数形式を厳密に定義する。モデルは説明と入力スキーマを読み取り、必要だと判断したときに呼び出す。AI SDK では generateText にツールを渡すだけで単発の関数呼び出しが可能になり、stopWhen 付きのループにすることで複数回ツールを呼び出すエージェントを構築できる ￼。prepareStep で履歴要約やメモリ圧縮を行い、onStepFinish でツール結果を保存するなど、コンテキスト管理の自由度が高い。

1.3 MCP（Model Context Protocol）との連携

MCP は Anthropic が提唱するオープンなプロトコルで、AI と外部システムをつなぐ ユニバーサルアダプター である。LLM に対し、利用可能なツールの名前・説明・入出力スキーマを含む ツールマニフェスト を標準形式で提供する ￼。モデルはマニフェストから使えるツールを理解し、適切なツール呼び出しを行う。MCP は標準的な HTTP（StreamableHTTP）や SSE、stdio などのトランスポートを用いてモデルとサーバーを接続する ￼。MCP サーバはツールを提供する側で、AI SDK には MCP クライアントを生成する機能 experimental_createMCPClient がある ￼。このクライアントは MCP サーバへ接続し、ツールスキーマを自動で取得する「スキーマディスカバリー」と、コード内で明示的にスキーマを定義する「スキーマ定義」の２つの方法でツールを登録できる ￼。

1.4 ツールを安全に取り込む：mcp-to-ai-sdk

MCP は外部サーバから動的にツール定義を取得できるが、そのままではセキュリティやスキーマ変更による不具合のリスクがある。Vercel の提案する mcp-to-ai-sdk CLI は、MCP サーバから取得したツール定義をローカルの TypeScript コードに ベンダリング（静的生成） することで、ソース管理下でバージョン管理やレビューを可能にする ￼。ベンダリングにより、不要なツールを除外してコンテキストを小さく保ち、プロンプトや引数の説明をカスタマイズできる。また、動的ロードを避けることでプロンプト注入や スキーマドリフト を防ぎ、信頼性を向上させる ￼。

2. コンテキスト管理とメモリのベストプラクティス

2.1 短期メモリと長期メモリ

LangGraph の文脈エンジニアリングのドキュメントは、エージェントにおけるメモリを以下のように分類している：
	•	短期メモリ：特定のタスクやスレッドに限定したコンテキスト。対話履歴や最新のツール出力を保持し、コンテキストウィンドウの制約に合わせて要約や削除を行う ￼。
	•	長期メモリ：会話をまたぐ知識やユーザープロファイルなどの永続データ。長期メモリは別のストレージ（ベクトルDBなど）に保存し、レトリーバルを介して必要なときに取り出す ￼。

2.2 コンテキストエンジニアリング

マルチエージェントでは、各サブエージェントが必要とするコンテキストを過不足なく共有することが成功の鍵である。LangChain のブログは、マルチエージェントが期待通りに動かない原因の多くが コンテキストの誤共有や不足 にあると指摘し、以下の原則を示している ￼：
	1.	全てのサブエージェントに必要な文脈を明示的に渡す。単純な分担ではなく、共同プロジェクトのリーダーが全員に最新情報を共有するイメージが必要。
	2.	決定権限や責務を明確にする。ツール呼び出しは行動そのものを意味するため、上位エージェントの意図と衝突しないよう整理する ￼。
	3.	タスクを分割する前に単一エージェントでの解決可能性を検討する。複数エージェントに分けるとコミュニケーションコストが増え、文脈工学の難易度が急上昇する ￼。

2.3 耐久実行とフォルトトレランス

長期実行や複数ツールを扱うエージェントは、システム障害や再試行に耐える必要がある。LangGraph の 耐久実行（durable execution） では、ワークフローの進捗をチェックポイントとして永続化し、再起動後に途中から再開できる設計を推奨している ￼。耐久実行には以下が重要：
	•	進捗を保存するチェックポインタ と 決定論的コード。これにより同じ入力で同じ状態へ必ず戻れる ￼。
	•	副作用のある操作（API 呼び出し等）はタスクとして分離し、冪等性（idempotency）を担保する ￼。
	•	失敗時のリトライポリシーとオブザーバビリティ：メトリクスやトレースを出力し、異常時に原因を特定しやすくする ￼。

3. MCP 詳説とエージェント統合

3.1 MCP の役割と利点

MCP は AI エージェントと外部サービスを結ぶ 共通言語 であり、従来の M×N 的な統合問題を M+N の関係に置き換える ￼。MCP は JSON‑RPC 2.0 上に構築され、以下の 3 つのコンポーネントを規定する ￼：
	•	MCP サーバ：ツール提供者が実装する。ツール・リソース・プロンプトを公開し、クライアントからのリクエストを実行する。
	•	MCP クライアント：AI アプリケーション側に組み込み、利用可能な MCP サーバを検出し、ツールマニフェストを理解・呼び出しする。
	•	ホストプロセス：モデルが稼働する環境で、エージェントのライフサイクル管理と MCP クライアントの利用を担う。

ツール以外にも、Resources（読み取り専用のデータソース）や Prompts（モデルが使うテンプレート）を標準化しており、さらに Sampling 機能によりモデルが複数候補を提示してホストが最適な選択を行うこともできる ￼。この統一的な設計は、複数ツールを使った長期タスク実行や他モデルとの協調に適している。

3.2 MCP と API の違い

API は人間やアプリケーションが直接呼び出すことを前提にしているのに対し、MCP サーバはモデルが理解できる形式でツールの説明とスキーマを提供し、モデルが自律的に適切なツールを選択できるようにする ￼。また、MCP サーバはデフォルトで権限やロジックを持たないため、ツールがアクセスできる範囲を開発者が厳密に定められる ￼。プロンプト注入などを防ぐためにも、ツールマニフェストをベンダリングし、不要なツールを除外するのが望ましい ￼。

3.3 MCP サーバの選択と利用方法

ローカル MCP サーバはモデルと同じ環境で動作し、プロトタイプやオフライン環境に向く。一方、リモート MCP サーバはクラウド上に配置され、複数のモデルやアプリから共通利用できる ￼。AI SDK には MCP クライアント生成 API があり、HTTP/SSE/stdio 向けのトランスポートを選択して接続する ￼。

4. マルチエージェント・オーケストレーションの潮流と論文サーベイ

4.1 マルチエージェントの設計パターン

LangGraph によるまとめでは、マルチエージェントは次のような構造が一般的である ￼：
	1.	ネットワーク型 – 各エージェントが同列に接続し、メッセージの受け渡しによって協調する。自由度は高いが、設計が煩雑になる。
	2.	スーパーバイザ型 – 監督エージェントが全体を管理し、サブエージェントへタスクを割り当てる。シンプルで一元的な制御が可能。
	3.	ツール呼び出し付きスーパーバイザ – サブエージェントが独自のツール呼び出しを持ち、スーパーバイザがその活用を調整する。
	4.	階層型 – タスクを分割して下層エージェントに渡し、結果を統合する。専門性を持たせられるが、コンテキストの伝達が難しい。

多くのフレームワークが提供するテンプレートでは、タスク分割と文脈共有をどこまで自動化するかが違う。LangChain のブログは「エージェント増加＝性能向上」という迷信を戒め、まず単一エージェントで解決可能か検討すること、マルチエージェントにする場合は 文脈工学 に十分な時間を割くことを勧めている ￼。

4.2 論文・実装事例
	•	Context Engineering for Multi‑Agent LLM Code Assistants (2025/08) – 複雑なコーディングタスクをマルチエージェント化し、Intent Translator（タスク分解）と Retrieval Agent を組み合わせる事で、単一エージェントよりも高い成功率を達成した ￼。この研究では、意図翻訳に GPT‑5 を用い、NotebookLM や Elicit などの情報取得ツールを統合。特定のコード処理を担当するサブエージェントを設け、適切な文脈を注入することでハルシネーションを抑えた ￼。
	•	AgentOrchestra: A Hierarchical Multi‑Agent Framework (2024/11) – 中央の計画エージェントがタスクを分解し、検索、コード編集、推論などの専門エージェントに仕事を割り振る階層型フレームワーク。実験では、単一エージェントや平坦なマルチエージェントより高い性能を示し、階層構造が効率的な協調に役立つことが示された ￼。
	•	Durable AI Loops (Restate blog) – エージェントを分散システムとして捉え、耐障害性を確保するためには進捗の永続化、冪等なツール呼び出し、リトライ制御が欠かせないと指摘。Vercel AI SDK や OpenAI Agents SDK にミドルウェア層を追加して durable execution を実現する例を示している ￼ ￼。

4.3 ベストプラクティスのまとめ
	1.	単独エージェントから始める – コンテキスト共有や調整コストを考慮し、まず単一エージェントでの実装を検討する。マルチエージェント化は必要最小限に留める ￼。
	2.	厳格なツール契約を定義する – OpenAI の関数呼び出しガイドラインに従い、ツールの入力スキーマを厳密に定義する。マルチエージェントでは特に入力エラーが連鎖しやすいため、Zod などのバリデーションを活用する。
	3.	コンテキストを意識的に設計する – どのサブエージェントに何を渡すかを明示し、要約・忘却・ベクトル検索を使い分けてコンテキストウィンドウを節約する ￼。LangGraph の context_schema や RAG（Retrieval Augmented Generation）技法を活用する。
	4.	耐久実行を組み込み、冪等性を担保する – チェックポイントやジョブログを用いて進捗を保存し、再試行時に同じ副作用を繰り返さないようにする ￼ ￼。Slack ボットのようなイベント処理では、event_id などで idempotency key を生成し、再送による重複実行を防ぐ。
	5.	ツールベンダリングとセキュリティ – MCP からツールを動的に読み込む場合は、mcp-to-ai-sdk でコード化して審査を通す。特に社内データベースや決済機能を公開する際は、アクセス権限と監査ログを整備する ￼。
	6.	観測性の確立 – エージェントの決定プロセス・ツール呼び出し・コストをログとして出力し、ダッシュボードで監視する。長期運用では Zapier タスク消費量や API レート制限を集計し、コストと性能のバランスを管理する。

5. まとめと所感

Vercel AI SDK のエージェントモードは、LLM と外部ツールの橋渡しを行うための強力なフレームワークだ。コアとなる ループ制御・コンテキスト管理・ツール呼び出しの設計は成熟しており、stopWhen や prepareStep 等のインターフェースで柔軟な制御が可能 ￼。また、MCP を利用することで、複数のアプリケーションやモデル間で再利用できるツールを公開しやすくなり、統合の手間を大きく減らせる ￼。ただし、マルチエージェント化は魔法の杖ではなく、文脈設計・耐久実行・セキュリティとコスト管理など、多くの配慮が必要である。研究や実装事例が示すように、意図を明確に分解し、必要な文脈を適切に共有し、冪等性を保つことが成功の鍵となる。

総合すると、2025 年現在のベストプラクティスは次のようにまとめられる：
	•	最低限必要なエージェント数から始め、文脈を意図的に設計する。
	•	MCP やツール定義をベンダリングし、セキュリティと信頼性を確保する。
	•	耐久実行と観測性を組み込み、運用コストやエラーを定量的に管理する。
	•	論文や先行事例を参考に、階層型・スーパーバイザ型などの構造を選択し、定義済みのスキーマと RAG を駆使してコンテキストを調整する。

以上の知見を踏まえれば、Vercel AI SDK のエージェントモードを利用した業務システムや Slack ボットを安全かつ効率的に構築できるはずだ。